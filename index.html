<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />
    <title>Haishan Ye</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
    <tr valign="top">
        <td id="layout-menu">
            <div class="menu-category">Haishan Ye</div>
            <div class="menu-item"><a href="index.html" class="current">Home</a></div>
        </td>
        <td id="layout-content">
            <div id="toptitle">
                <h1>Haishan Ye
                </h1>
                <div id="subtitle">
                </div>
            </div>
            <table class="imgtable">
                <tr>
                <td><img src="images/images.jpg" alt="" width="150" height="250"/>&nbsp;</td>
                <td align="left"><p>Associate Professor<br />
                    <p>School of Management<br />
                    </p>Xi'an JiaoTong University<br /><br />
                    </p>
                    <h2>Contact
                    </h2>
                    <p>yehaishan@xjtu.edu.cn
                    </p>
                </td>
                </tr>
            </table>
            <h2>About Me
            </h2>
            <p>I am now an Associate Professor at School of Management, Xi'an JiaoTong University. I was a Postdoctoral Researcher in the Hongkong University of Science and Technology.
             I am also a Research Scientist in A*STAR CFAR-SGIT AI Lab. My current research interest is Machine Learning, optimization algorithm and numerical linear algebra.
            </p>
            <p>
                <span style="color:rgba(255,47,0,0.73)">Note:Our lab recruits long-term interns for machine learning and visual computing positions, and will provide abundant GPU computing resources
                    and <b>generous living subsidies during the internship: 2-3k/month for undergraduate students、5-6k/month for master students and 6-8k/month for doctoral students</b>. In addition, our working hours are flexible. For more information, please click on the link below, and we welcome everyone to join the laboratory.
                </span>
            </p>
                (<a href="https://mp.weixin.qq.com/s/SKu28m2KTalcFBjO6wRCnA">新加坡A*STAR-国网思机未来人工智能(SGIT AI)联合研究实习岗位开放</a>)


            <h2>Publications (*Corresponding author)
            </h2>

            <h3>Conference Publications
            </h3>
            <ul>
                <li>Dachao Lin, Yuze Han, <b>Haishan Ye</b>*, Zhihua Zhang.<br>
                    Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis.<br>
                    Advances in Neural Information Processing Systems(NeurIPS),2024.<br>
                    [<a href="https://arxiv.org/pdf/2304.07504.pdf">pdf</a>]
                </li>
                <li>Jun Chen, <b>Haishan Ye</b>, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor W Tsang, Yong Liu.<br>
                    Decentralized Riemannian conjugate gradient method on the Stiefel manifold.<br>
                    International Conference on Learning Representations(ICLR),2024.<br>
                    [<a href="https://arxiv.org/pdf/2308.10547.pdf">pdf</a>]
                </li>
                <li>Luo Luo, Cheng Chen, Guangzeng Xie, <b>Haishan Ye</b>.<br>
                    Revisiting Co-Occurring Directions: Sharper Analysis and Efficient Algorithm for Sparse Matrices.<br>
                    Proceedings of the AAAI Conference on Artificial Intelligence(AAAI),2021.<br>
                    [<a href="https://arxiv.org/pdf/2009.02553.pdf">pdf</a>]
                </li>
                <li>Rui Pan, <b>Haishan Ye</b>*, Tong Zhang.<br>
                    Eigencurve: Optimal learning rate schedule for sgd on quadratic objectives with skewed hessian spectrums.<br>
                    International Conference on Learning Representations(ICLR),2021.<br>
                    [<a href="https://arxiv.org/pdf/2110.14109.pdf">pdf</a>]
                </li>
                <li>Dachao Lin, <b>Haishan Ye</b>, Zhihua Zhang.<br>
                    Greedy and random quasi-newton methods with faster explicit superlinear convergence.<br>
                    Advances in Neural Information Processing Systems(NeurIPS),2021.<br>
                    [<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/347665597cbfaef834886adbb848011f-Paper.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Ziang Zhou, Luo Luo, Tong Zhang.<br>
                    Decentralized accelerated proximal gradient descent.<br>
                    Advances in Neural Information Processing Systems(NeurIPS),2020.<br>
                    [<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/d4b5b5c16df28e61124e13181db7774c-Paper.pdf">pdf</a>]
                </li>
                <li>Luo Luo, <b>Haishan Ye</b>, Zhichao Huang, Tong Zhang.<br>
                    Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems.<br>
                    Advances in Neural Information Processing Systems(NeurIPS),2020.<br>
                    [<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/ecb47fbb07a752413640f82a945530f8-Paper.pdf">pdf</a>]
                </li>
                <li>Chaoyang He, <b>Haishan Ye</b>, Li Shen, Tong Zhang.<br>
                    Milenas: Efficient neural architecture search via mixed-level reformulation.<br>
                    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(IEEE/CVF),2020.<br>
                    [<a href="https://arxiv.org/pdf/2003.12238.pdf">pdf</a>]
                </li>
                <li>Yujun Li, Kaichun Mo, <b>Haishan Ye</b>.
                    Accelerating random Kaczmarz algorithm based on clustering information.<br>
                    Proceedings of the AAAI Conference on Artificial Intelligence(AAAI),2016.<br>
                    [<a href="https://arxiv.org/pdf/1511.05362.pdf">pdf</a>]
                </li>
            </ul>
            <h3>Journal Publications
            </h3>
            <ul>
                <li>Jun Shang, <b>Haishan Ye</b>, Xiangyu Chang.<br>
                    Accelerated double-sketching subspace Newton.<br>
                    European Journal of Operational Research(EJOR),2024.<br>
                    [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221724002613">link</a>]
                </li>
                <li><b>Haishan Ye</b>, Luo Luo, Ziang Zhou, Tong Zhang.<br>
                    Multi-consensus decentralized accelerated gradient descent.<br>
                    Journal of Machine Learning Research(JMLR),2023.<br>
                    [<a href="https://arxiv.org/pdf/2005.00797.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Dachao Lin, Xiangyu Chang, Zhihua Zhang.<br>
                    Towards explicit superlinear convergence rate for SR1.<br>
                    Mathematical Programming,2023.<br>
                    [<a href="https://link.springer.com/article/10.1007/s10107-022-01865-w">link</a>]
                </li>
                <li><b>Haishan Ye</b>, Shiyuan He, Xiangyu Chang.<br>
                    DINE: Decentralized Inexact Newton With Exact Linear Convergence Rate.<br>
                    IEEE Transactions on Signal Processing,2023.<br>
                    [<a href="https://ieeexplore.ieee.org/abstract/document/10330000">link</a>]
                </li>
                <li>Dachao Lin, <b>Haishan Ye</b>*, Zhihua Zhang.<br>
                    Explicit convergence rates of greedy and random quasi-Newton methods.<br>
                    Journal of Machine Learning Research(JMLR),2022.<br>
                    [<a href="https://www.jmlr.org/papers/volume23/21-1282/21-1282.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Chaoyang He, Xiangyu Chang.<br>
                    Accelerated distributed approximate Newton method.<br>
                    IEEE Transactions on Neural Networks and Learning Systems,2022.<br>
                    [<a href="https://ieeexplore.ieee.org/document/9729543">link</a>]
                </li>
                <li><b>Haishan Ye</b>, Luo Luo, Zhihua Zhang.<br>
                    Approximate newton methods.<br>
                    Journal of Machine Learning Research(JMLR),2021.<br>
                    [<a href="https://jmlr.org/papers/volume22/19-870/19-870.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>*, Tong Zhang.<br>
                    DeEPCA: Decentralized exact PCA with linear convergence rate.<br>
                    Journal of Machine Learning Research(JMLR),2021.<br>
                    [<a href="https://www.jmlr.org/papers/volume22/21-0298/21-0298.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Luo Luo, Zhihua Zhang.<br>
                    Nesterov's acceleration for approximate Newton.<br>
                    Journal of Machine Learning Research(JMLR),2020.<br>
                    [<a href="https://www.jmlr.org/papers/volume21/19-265/19-265.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Luo Luo, Zhihua Zhang.<br>
                    Accelerated proximal subsampled Newton method.<br>
                    IEEE Transactions on Neural Networks and Learning Systems,2020.<br>
                    [<a href="https://ieeexplore.ieee.org/document/9189826">link</a>]
                </li>
                <li><b>Haishan Ye</b>, Guangzeng Xie, Luo Luo, Zhihua Zhang.<br>
                    Fast stochastic second-order method logarithmic in condition number.<br>
                    Pattern Recognition,2019.<br>
                    [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320318304163">link</a>]
                </li>
                <li><b>Haishan Ye</b>, Yujun Li, Cheng Chen, Zhihua Zhang.<br>
                    Fast Fisher discriminant analysis with randomized algorithms.<br>
                    Pattern Recognition,2017.<br>
                    [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320317302509">link</a>]
                </li>
            </ul>
            <h3>Preprints
            </h3>
            <ul>
                <li><b>Haishan Ye</b>, Dachao Lin, Xiangyu Chang, Zhihua Zhang.<br>
                    Anderson Acceleration Without Restart: A Novel Method with n-Step Super Quadratic Convergence Rate.<br>
                    arXiv preprint arXiv:2403.16734.<br>
                    [<a href="https://arxiv.org/pdf/2403.16734.pdf">pdf</a>]
                </li>
                <li>Yanjun Zhao, Sizhe Dang,<b>Haishan Ye</b>*,Guang Dai, Yi Qian, Ivor W Tsang.<br>
                    Second-Order Fine-Tuning without Pain for LLMs: A Hessian Informed Zeroth-Order Optimizer.<br>
                    arXiv preprint arXiv:2402.15173.<br>
                    [<a href="https://arxiv.org/pdf/2402.15173.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>,Xiangyu Chang.<br>
                    Optimal Decentralized Composite Optimization for Strongly Convex Functions.<br>
                    arXiv preprint arXiv:2312.15845.<br>
                    [<a href="https://arxiv.org/pdf/2312.15845.pdf">pdf</a>]
                </li>
                <li>Hao Di, Yi Yang, <b>Haishan Ye</b>, Xiangyu Chang.<br>
                    PPFL: A Personalized Federated Learning Framework for Heterogeneous Population.<br>
                    arXiv preprint arXiv:2310.14337.<br>
                    [<a href="https://arxiv.org/pdf/2310.14337.pdf">pdf</a>]
                </li>

                <li><b>Haishan Ye</b>.<br>
                    Mirror natural evolution strategies.<br>
                    arXiv preprint arXiv:2308.00469.<br>
                    [<a href="https://arxiv.org/pdf/2308.00469.pdf">pdf</a>]
                </li>
                <li>Lesi Chen, <b>Haishan Ye</b>, Luo Luo.<br>
                    An Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization.<br>
                    arXiv preprint arXiv:2212.02387.<br>
                    [<a href="https://arxiv.org/pdf/2212.02387.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Xiangyu Chang.<br>
                    Snap-shot decentralized stochastic gradient tracking methods.<br>
                    arXiv preprint arXiv:2212.05273.<br>
                    [<a href="https://arxiv.org/pdf/2212.05273.pdf">pdf</a>]
                </li>
                <li>Luo Luo, <b>Haishan Ye</b>.<br>
                    An optimal stochastic algorithm for decentralized nonconvex finite-sum optimization.<br>
                    arXiv preprint arXiv:2210.13931.<br>
                    [<a href="https://arxiv.org/pdf/2210.13931.pdf">pdf</a>]
                </li>
                <li>Luo Luo, <b>Haishan Ye</b>.<br>
                    Decentralized stochastic variance reduced extragradient method.<br>
                    arXiv preprint arXiv:2202.00509.<br>
                    [<a href="https://arxiv.org/pdf/2202.00509.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Dachao Lin, Zhihua Zhang.<br>
                    Greedy and Random Broyden's Methods with Explicit Superlinear Convergence Rates in Nonlinear Equations.<br>
                    arXiv preprint arXiv:2110.08572.<br>
                    [<a href="https://arxiv.org/pdf/2110.08572.pdf">pdf</a>]
                </li>

                <li>Dachao Lin, <b>Haishan Ye</b>, Zhihua Zhang.<br>
                    Explicit superlinear convergence rates of Broyden's methods in nonlinear equations.<br>
                    arXiv preprint arXiv:2109.01974.<br>
                    [<a href="https://arxiv.org/pdf/2109.01974.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Shusen Wang, Zhihua Zhang, Tong Zhang.<br>
                    Fast Generalized Matrix Regression with Applications in Machine Learning.<br>
                    arXiv preprint arXiv:1912.12008.<br>
                    [<a href="https://arxiv.org/pdf/1912.12008.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>, Zhichao Huang, Cong Fang, Chris Junchi Li, Tong Zhang.<br>
                    Hessian-aware zeroth-order optimization for black-box adversarial attack.<br>
                    arXiv preprint arXiv:1812.11377.<br>
                    [<a href="https://arxiv.org/pdf/1812.11377.pdf">pdf</a>]
                </li>
                <li><b>Haishan Ye</b>,Wei Xiong, Tong Zhang.<br>
                    PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction.<br>
                    arXiv preprint arXiv:2012.15010.<br>
                    [<a href="https://arxiv.org/pdf/2012.15010.pdf">pdf</a>]
                </li>
            </ul>
            <hr>
        </td>
    </tr>
</table>



</body>
</html>